# Measuring-IA-Quality-Evaluation-&-Creation-of-Automatic-summaries.

This project explores AI quality measurement through the creation of automatic summaries using GPTâ€‘4, using the Dataset *CNN/DailyMail*, a classic benchmark for summarization tasks.
The aproach of the LLM model is to generate summaries compared against human references, for evaluate this model automatic metrics (BLEU, ROUGE, METEOR, BERTScore) were applied to assess semantic and lexical quality.
Also a Linguistic Analysis was made, to do this a comparative POS tagging study was conducted to evaluate grammatical and stylistic consistency.

---

## ðŸ“‘ Table of Contents
- Input and Cleaning
- Generation of the Large Lenguage model
- Evaluation metrics
  -  Recall-Oriented Understudy for Gisting Evaluation (ROUGE)
  -  Bilingual Evaluation Understudy (BLEU)
  -  BERTscore (Bidirectional Encoder Representations from Transformers)
- POS tagging analysis (Part-of-Speech)

---

## ðŸ§° Technologies and Tools

- Python
- Datasets
- LLM
- GPT4ALL
- Pandas
- tqdm
- Hugging Face tools: evaluate, datasets, transformers
- nltk
- collections
- matplotlib

---

## ðŸ“‚ How to Use This Repository

You can explore the full analysis, visualizations, and conclusions downloading the PDF document:  
**[Measuring IA Quality: Evaluation & Creation of Automatic summaries](https://)**  
This version was exported directly from the original Jupyter Notebook using LaTeX.

You may also browse the `/img` folder to preview the key plot of the semantic analysis.

